#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Gushen AI - é¾™è™æ¦œæ•°æ®å¤„ç†å®Œæ•´æµæ°´çº¿
ä¸²è”æ•°æ®è·å–ã€å¤„ç†ã€æå–å’ŒLLMåˆ†æçš„å®Œæ•´workflow

Version: 1.0
Author: AI  
Date: 2025-01-XX
"""

import os
import sys
import json
import logging
import argparse
import threading
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
import time

# æ·»åŠ coreç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'core'))
sys.path.append(os.path.join(os.path.dirname(__file__), 'utils'))

from core.data_fetcher import DataFetcher
from core.data_processor import DataProcessor  
from utils.stock_data_extractor import StockDataExtractor
from core.funding_battle_analyzer import FundingBattleAnalyzer
from core.post_generator_v2 import PostGeneratorV2

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/dragon_tiger_pipeline.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('dragon_tiger_pipeline')


class DragonTigerPipeline:
    """
    é¾™è™æ¦œæ•°æ®å¤„ç†å®Œæ•´æµæ°´çº¿
    å°†æ•°æ®è·å–ã€å¤„ç†ã€æå–å’ŒLLMåˆ†æä¸²è”èµ·æ¥
    """
    
    def __init__(self, tushare_token: str = None, max_workers: int = 16, api_delay: float = 0.1, enable_post_generation: bool = False):
        """
        åˆå§‹åŒ–æµæ°´çº¿
        
        Args:
            tushare_token: Tushare API tokenï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å€¼
            max_workers: æœ€å¤§å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°ï¼ˆDeepSeekæ— é™åˆ¶ï¼Œå¯ä»¥æ¿€è¿›è®¾ç½®ï¼‰
            api_delay: APIè°ƒç”¨é—´éš”ï¼ˆç§’ï¼‰ï¼Œç”±äºæ— é™åˆ¶å¯ä»¥è®¾ç½®å¾ˆå°
            enable_post_generation: æ˜¯å¦å¯ç”¨å¸–å­ç”ŸæˆåŠŸèƒ½
        """
        logger.info("åˆå§‹åŒ–é¾™è™æ¦œæ•°æ®å¤„ç†æµæ°´çº¿ï¼ˆé«˜å¹¶å‘ç‰ˆæœ¬+å¸–å­ç”Ÿæˆï¼‰...")
        
        # åˆå§‹åŒ–å„ä¸ªç»„ä»¶
        self.data_fetcher = DataFetcher(tushare_token)
        self.data_processor = DataProcessor(self.data_fetcher)
        
        # å¸–å­ç”Ÿæˆé…ç½®
        self.enable_post_generation = enable_post_generation
        if enable_post_generation:
            self.post_generator = PostGeneratorV2()
            logger.info("âœ… å¸–å­ç”ŸæˆåŠŸèƒ½å·²å¯ç”¨")
        
        # é«˜å¹¶å‘é…ç½®
        self.max_workers = max_workers
        self.api_delay = api_delay
        
        # çº¿ç¨‹å®‰å…¨é”
        self.progress_lock = Lock()
        self.result_lock = Lock()
        
        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        self._ensure_directories()
        
        logger.info(f"æµæ°´çº¿åˆå§‹åŒ–å®Œæˆï¼Œé«˜å¹¶å‘æ¨¡å¼: {max_workers}çº¿ç¨‹, APIå»¶è¿Ÿ: {api_delay}ç§’")
    
    def _ensure_directories(self):
        """ç¡®ä¿å¿…è¦çš„ç›®å½•å­˜åœ¨"""
        directories = [
            'data/processed',
            'data/extracted', 
            'data/analyzed',
            'data/output/posts',  # å¸–å­è¾“å‡ºæ ¹ç›®å½•
            'logs'
        ]
        
        for directory in directories:
            os.makedirs(directory, exist_ok=True)
    
    def _ensure_post_date_directory(self, trade_date: str):
        """ç¡®ä¿æŒ‡å®šæ—¥æœŸçš„å¸–å­ç›®å½•å­˜åœ¨"""
        post_date_dir = f"data/output/posts/{trade_date}"
        os.makedirs(post_date_dir, exist_ok=True)
        return post_date_dir
    
    def _ensure_date_directory(self, trade_date: str):
        """ç¡®ä¿æŒ‡å®šæ—¥æœŸçš„ç›®å½•å­˜åœ¨"""
        date_dir = f"data/analyzed/{trade_date}"
        os.makedirs(date_dir, exist_ok=True)
        return date_dir
    
    def process_date(self, trade_date: str, skip_existing: bool = True) -> Dict[str, Any]:
        """
        å¤„ç†æŒ‡å®šæ—¥æœŸçš„å®Œæ•´é¾™è™æ¦œæ•°æ®
        
        Args:
            trade_date: äº¤æ˜“æ—¥æœŸï¼Œæ ¼å¼YYYYMMDD
            skip_existing: æ˜¯å¦è·³è¿‡å·²å­˜åœ¨çš„åˆ†æç»“æœ
            
        Returns:
            åŒ…å«æ‰€æœ‰è‚¡ç¥¨åˆ†æç»“æœçš„å­—å…¸
        """
        logger.info(f"å¼€å§‹å¤„ç†{trade_date}çš„é¾™è™æ¦œæ•°æ®")
        
        # æ­¥éª¤1ï¼šæ•°æ®è·å–å’Œå¤„ç†
        processed_data = self._step1_fetch_and_process(trade_date)
        if not processed_data:
            return {"error": "æ•°æ®è·å–æˆ–å¤„ç†å¤±è´¥"}
        
        # æ­¥éª¤2ï¼šæå–å’Œåˆ†ææ¯åªè‚¡ç¥¨ï¼ˆæ ¹æ®æ¨¡å¼é€‰æ‹©å¹¶è¡Œæˆ–ä¸²è¡Œï¼‰
        if hasattr(self, '_force_serial') and self._force_serial:
            analysis_results = self._step2_analyze_stocks_serial(processed_data, trade_date, skip_existing)
        else:
            analysis_results = self._step2_analyze_stocks_parallel(processed_data, trade_date, skip_existing)
        
        # æ­¥éª¤3ï¼šç”Ÿæˆå¸–å­ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.enable_post_generation and analysis_results:
            if hasattr(self, '_force_serial') and self._force_serial:
                post_results = self._step3_generate_posts_serial(analysis_results, trade_date, skip_existing)
            else:
                post_results = self._step3_generate_posts_parallel(analysis_results, trade_date, skip_existing)
        else:
            post_results = []
        
        # æ­¥éª¤4ï¼šæ±‡æ€»ç»“æœ
        summary_result = self._step4_generate_summary(analysis_results, post_results, trade_date)
        
        logger.info(f"å®Œæˆ{trade_date}çš„é¾™è™æ¦œæ•°æ®å¤„ç†")
        return summary_result
    
    def _step1_fetch_and_process(self, trade_date: str) -> Optional[Dict[str, Any]]:
        """
        æ­¥éª¤1ï¼šè·å–åŸå§‹æ•°æ®å¹¶è¿›è¡Œé¢„å¤„ç†
        """
        try:
            logger.info("æ­¥éª¤1: è·å–å’Œå¤„ç†åŸå§‹æ•°æ®")
            
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰å¤„ç†åçš„æ•°æ®
            processed_file = f"data/processed/{trade_date}_processed_data.json"
            if os.path.exists(processed_file):
                logger.info(f"å‘ç°å·²å¤„ç†çš„æ•°æ®æ–‡ä»¶: {processed_file}")
                with open(processed_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            
            # è·å–å’Œå¤„ç†æ•°æ®
            processed_data = self.data_processor.process_single_date_data(
                trade_date=trade_date,
                days_back=10
            )
            
            if not processed_data or 'error' in processed_data:
                logger.error(f"æ•°æ®å¤„ç†å¤±è´¥: {processed_data}")
                return None
            
            # ä¿å­˜å¤„ç†åçš„æ•°æ®
            with open(processed_file, 'w', encoding='utf-8') as f:
                json.dump(processed_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"æ•°æ®å¤„ç†å®Œæˆï¼ŒåŒ…å«{processed_data['meta']['stock_count']}åªè‚¡ç¥¨")
            return processed_data
            
        except Exception as e:
            logger.error(f"æ­¥éª¤1å¤±è´¥: {e}")
            return None
    
    def _step2_analyze_stocks_serial(self, processed_data: Dict[str, Any], trade_date: str, skip_existing: bool) -> List[Dict[str, Any]]:
        """
        æ­¥éª¤2ï¼šæå–å¹¶åˆ†ææ¯åªè‚¡ç¥¨
        """
        logger.info("æ­¥éª¤2: åˆ†æå„åªè‚¡ç¥¨")
        
        # ç¡®ä¿æ—¥æœŸç›®å½•å­˜åœ¨
        date_dir = self._ensure_date_directory(trade_date)
        
        stocks = processed_data.get('stocks', [])
        analysis_results = []
        
        # ä½¿ç”¨è¿›åº¦æ¡æ˜¾ç¤ºå¤„ç†è¿›åº¦
        with tqdm(total=len(stocks), desc="åˆ†æè‚¡ç¥¨") as pbar:
            for i, stock_data in enumerate(stocks):
                stock_name = stock_data.get('name', 'Unknown')
                ts_code = stock_data.get('ts_code', 'Unknown')
                
                pbar.set_description(f"åˆ†æ {stock_name}")
                
                try:
                    # æ„å»ºåˆ†æç»“æœæ–‡ä»¶è·¯å¾„ï¼ˆæ”¾åœ¨æ—¥æœŸç›®å½•ä¸‹ï¼‰
                    analysis_file = os.path.join(date_dir, f"{stock_name}_{ts_code.replace('.', '_')}_analysis.json")
                    
                    if skip_existing and os.path.exists(analysis_file):
                        logger.info(f"è·³è¿‡å·²åˆ†æçš„è‚¡ç¥¨: {stock_name}")
                        # åŠ è½½å·²æœ‰ç»“æœ
                        with open(analysis_file, 'r', encoding='utf-8') as f:
                            existing_result = json.load(f)
                            analysis_results.append(existing_result)
                        pbar.update(1)
                        continue
                    
                    # æå–å•è‚¡æ•°æ®
                    extracted_data = self._extract_single_stock(processed_data, stock_data, trade_date)
                    if not extracted_data:
                        logger.warning(f"æå–{stock_name}æ•°æ®å¤±è´¥")
                        pbar.update(1)
                        continue
                    
                    # LLMåˆ†æ
                    analysis_result = self._analyze_single_stock(extracted_data, stock_name, ts_code)
                    if analysis_result:
                        # ä¿å­˜åˆ†æç»“æœåˆ°æ—¥æœŸç›®å½•ä¸‹
                        with open(analysis_file, 'w', encoding='utf-8') as f:
                            json.dump(analysis_result, f, ensure_ascii=False, indent=2)
                        
                        analysis_results.append(analysis_result)
                        logger.info(f"å®Œæˆ{stock_name}åˆ†æï¼Œä¿å­˜è‡³: {analysis_file}")
                    else:
                        logger.warning(f"åˆ†æ{stock_name}å¤±è´¥")
                
                except Exception as e:
                    logger.error(f"å¤„ç†{stock_name}æ—¶å‡ºé”™: {e}")
                
                pbar.update(1)
        
        return analysis_results
    
    def _step2_analyze_stocks_parallel(self, processed_data: Dict[str, Any], trade_date: str, skip_existing: bool) -> List[Dict[str, Any]]:
        """
        æ­¥éª¤2ï¼šå¹¶è¡Œæå–å¹¶åˆ†ææ¯åªè‚¡ç¥¨ï¼ˆé«˜å¹¶å‘ç‰ˆæœ¬ï¼‰
        """
        logger.info(f"æ­¥éª¤2: é«˜å¹¶å‘åˆ†æå„åªè‚¡ç¥¨ï¼ˆ{self.max_workers}çº¿ç¨‹ï¼‰")
        
        # ç¡®ä¿æ—¥æœŸç›®å½•å­˜åœ¨
        date_dir = self._ensure_date_directory(trade_date)
        
        stocks = processed_data.get('stocks', [])
        analysis_results = []
        
        # å‡†å¤‡ä»»åŠ¡å‚æ•°
        tasks = [(stock_data, trade_date, date_dir, skip_existing, processed_data) for stock_data in stocks]
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_stock = {
                executor.submit(self._analyze_single_stock_worker, task): task[0]['name'] 
                for task in tasks
            }
            
            # ä½¿ç”¨è¿›åº¦æ¡æ˜¾ç¤ºå¤„ç†è¿›åº¦
            with tqdm(total=len(stocks), desc="ğŸš€ é«˜å¹¶å‘åˆ†æä¸­") as pbar:
                for future in as_completed(future_to_stock):
                    stock_name = future_to_stock[future]
                    try:
                        result = future.result()
                        if result:
                            with self.result_lock:
                                analysis_results.append(result)
                    except Exception as e:
                        logger.error(f"åˆ†æ{stock_name}æ—¶å‡ºç°å¼‚å¸¸: {e}")
                    
                    with self.progress_lock:
                        pbar.update(1)
                        pbar.set_description(f"ğŸš€ å·²å®Œæˆ {len(analysis_results)} åªè‚¡ç¥¨")
        
        logger.info(f"é«˜å¹¶å‘åˆ†æå®Œæˆï¼ŒæˆåŠŸåˆ†æ {len(analysis_results)}/{len(stocks)} åªè‚¡ç¥¨")
        return analysis_results
    
    def _analyze_single_stock_worker(self, args: Tuple[Dict[str, Any], str, str, bool, Dict[str, Any]]) -> Optional[Dict[str, Any]]:
        """
        å•åªè‚¡ç¥¨åˆ†æçš„å·¥ä½œå‡½æ•°ï¼ˆçº¿ç¨‹å·¥ä½œå•å…ƒï¼‰
        
        Args:
            args: (stock_data, trade_date, date_dir, skip_existing, processed_data)
            
        Returns:
            åˆ†æç»“æœæˆ–None
        """
        stock_data, trade_date, date_dir, skip_existing, processed_data = args
        
        stock_name = stock_data.get('name', 'Unknown')
        ts_code = stock_data.get('ts_code', 'Unknown')
        thread_id = threading.current_thread().ident
        
        try:
            # æ„å»ºåˆ†æç»“æœæ–‡ä»¶è·¯å¾„
            analysis_file = os.path.join(date_dir, f"{stock_name}_{ts_code.replace('.', '_')}_analysis.json")
            
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰åˆ†æç»“æœ
            if skip_existing and os.path.exists(analysis_file):
                logger.debug(f"[çº¿ç¨‹{thread_id}] è·³è¿‡å·²åˆ†æçš„è‚¡ç¥¨: {stock_name}")
                with open(analysis_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            
            # æå–å•è‚¡æ•°æ®ï¼ˆçº¿ç¨‹å®‰å…¨ç‰ˆæœ¬ï¼‰
            extracted_file = self._extract_single_stock_for_thread(stock_data, trade_date, thread_id, processed_data)
            if not extracted_file:
                logger.warning(f"[çº¿ç¨‹{thread_id}] æå–{stock_name}æ•°æ®å¤±è´¥")
                return None
            
            # è½»å¾®çš„APIè°ƒç”¨å»¶è¿Ÿï¼ˆé˜²æ­¢è¿‡åº¦å¹¶å‘ï¼‰
            if self.api_delay > 0:
                time.sleep(self.api_delay)
            
            # åˆ›å»ºçº¿ç¨‹ä¸“ç”¨çš„åˆ†æå™¨å®ä¾‹
            funding_analyzer = FundingBattleAnalyzer()
            
            # LLMåˆ†æ
            analysis_result = funding_analyzer.analyze_complete_report(
                data_file_path=extracted_file,
                output_path=None
            )
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(extracted_file):
                os.remove(extracted_file)
            
            if analysis_result:
                # ä¿å­˜åˆ†æç»“æœ
                with open(analysis_file, 'w', encoding='utf-8') as f:
                    json.dump(analysis_result, f, ensure_ascii=False, indent=2)
                
                analysis_result['processed_at'] = datetime.now().isoformat()
                logger.debug(f"[çº¿ç¨‹{thread_id}] å®Œæˆ{stock_name}åˆ†æ")
                return analysis_result
            else:
                logger.warning(f"[çº¿ç¨‹{thread_id}] åˆ†æ{stock_name}å¤±è´¥")
                return None
                
        except Exception as e:
            logger.error(f"[çº¿ç¨‹{thread_id}] å¤„ç†{stock_name}æ—¶å‡ºé”™: {e}")
            return None
    
    def _extract_single_stock_for_thread(self, stock_data: Dict[str, Any], trade_date: str, thread_id: int, processed_data: Dict[str, Any]) -> Optional[str]:
        """
        ä¸ºçº¿ç¨‹æå–å•åªè‚¡ç¥¨çš„æ•°æ®ï¼ˆçº¿ç¨‹å®‰å…¨ç‰ˆæœ¬ï¼‰
        """
        try:
            # åˆ›å»ºä¸´æ—¶çš„å®Œæ•´æ•°æ®ç»“æ„
            temp_data = {
                "meta": processed_data["meta"],
                "stocks": [stock_data]
            }
            
            # ä¿å­˜ä¸´æ—¶æ–‡ä»¶ï¼ˆåŒ…å«çº¿ç¨‹IDé¿å…å†²çªï¼‰
            temp_file = f"data/extracted/temp_{trade_date}_{stock_data['name']}_{stock_data['ts_code'].replace('.', '_')}_thread{thread_id}.json"
            with open(temp_file, 'w', encoding='utf-8') as f:
                json.dump(temp_data, f, ensure_ascii=False, indent=2)
            
            return temp_file
            
        except Exception as e:
            logger.error(f"[çº¿ç¨‹{thread_id}] æå–å•è‚¡æ•°æ®å¤±è´¥: {e}")
            return None
    
    def _step3_generate_posts_parallel(self, analysis_results: List[Dict[str, Any]], trade_date: str, skip_existing: bool) -> List[Dict[str, Any]]:
        """
        æ­¥éª¤3ï¼šå¹¶è¡Œç”Ÿæˆå¸–å­ï¼ˆé«˜å¹¶å‘ç‰ˆæœ¬ï¼‰
        """
        if not self.enable_post_generation:
            logger.info("å¸–å­ç”ŸæˆåŠŸèƒ½æœªå¯ç”¨ï¼Œè·³è¿‡")
            return []
        
        logger.info(f"æ­¥éª¤3: é«˜å¹¶å‘ç”Ÿæˆå¸–å­ï¼ˆ{self.max_workers}çº¿ç¨‹ï¼‰")
        
        # ç¡®ä¿æ—¥æœŸç›®å½•å­˜åœ¨
        date_dir = self._ensure_date_directory(trade_date)
        post_date_dir = self._ensure_post_date_directory(trade_date)
        
        # è¿‡æ»¤æœ‰æ•ˆçš„åˆ†æç»“æœ
        valid_results = [r for r in analysis_results if r and 'analysis_report' in r]
        post_results = []
        
        # å‡†å¤‡ä»»åŠ¡å‚æ•°
        tasks = [(result, trade_date, date_dir, post_date_dir, skip_existing) for result in valid_results]
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_stock = {
                executor.submit(self._generate_single_post_worker, task): task[0].get('stock_info', {}).get('name', 'Unknown')
                for task in tasks
            }
            
            # ä½¿ç”¨è¿›åº¦æ¡æ˜¾ç¤ºå¤„ç†è¿›åº¦
            with tqdm(total=len(valid_results), desc="ğŸ“ é«˜å¹¶å‘ç”Ÿæˆå¸–å­") as pbar:
                for future in as_completed(future_to_stock):
                    stock_name = future_to_stock[future]
                    try:
                        result = future.result()
                        if result:
                            with self.result_lock:
                                post_results.append(result)
                    except Exception as e:
                        logger.error(f"ç”Ÿæˆ{stock_name}å¸–å­æ—¶å‡ºç°å¼‚å¸¸: {e}")
                    
                    with self.progress_lock:
                        pbar.update(1)
                        pbar.set_description(f"ğŸ“ å·²ç”Ÿæˆ {len(post_results)} ç¯‡å¸–å­")
        
        logger.info(f"é«˜å¹¶å‘å¸–å­ç”Ÿæˆå®Œæˆï¼ŒæˆåŠŸç”Ÿæˆ {len(post_results)}/{len(valid_results)} ç¯‡å¸–å­")
        return post_results
    
    def _step3_generate_posts_serial(self, analysis_results: List[Dict[str, Any]], trade_date: str, skip_existing: bool) -> List[Dict[str, Any]]:
        """
        æ­¥éª¤3ï¼šä¸²è¡Œç”Ÿæˆå¸–å­
        """
        if not self.enable_post_generation:
            logger.info("å¸–å­ç”ŸæˆåŠŸèƒ½æœªå¯ç”¨ï¼Œè·³è¿‡")
            return []
        
        logger.info("æ­¥éª¤3: ä¸²è¡Œç”Ÿæˆå¸–å­")
        
        # ç¡®ä¿æ—¥æœŸç›®å½•å­˜åœ¨
        date_dir = self._ensure_date_directory(trade_date)
        post_date_dir = self._ensure_post_date_directory(trade_date)
        
        # è¿‡æ»¤æœ‰æ•ˆçš„åˆ†æç»“æœ
        valid_results = [r for r in analysis_results if r and 'analysis_report' in r]
        post_results = []
        
        # ä½¿ç”¨è¿›åº¦æ¡æ˜¾ç¤ºå¤„ç†è¿›åº¦
        with tqdm(total=len(valid_results), desc="ğŸ“ ç”Ÿæˆå¸–å­") as pbar:
            for result in valid_results:
                stock_name = result.get('stock_info', {}).get('name', 'Unknown')
                pbar.set_description(f"ç”Ÿæˆ {stock_name} å¸–å­")
                
                try:
                    post_result = self._generate_single_post_worker((result, trade_date, date_dir, post_date_dir, skip_existing))
                    if post_result:
                        post_results.append(post_result)
                        logger.info(f"å®Œæˆ{stock_name}å¸–å­ç”Ÿæˆ")
                    else:
                        logger.warning(f"ç”Ÿæˆ{stock_name}å¸–å­å¤±è´¥")
                
                except Exception as e:
                    logger.error(f"å¤„ç†{stock_name}å¸–å­æ—¶å‡ºé”™: {e}")
                
                pbar.update(1)
        
        return post_results
    
    def _generate_single_post_worker(self, args: Tuple[Dict[str, Any], str, str, str, bool]) -> Optional[Dict[str, Any]]:
        """
        å•ä¸ªå¸–å­ç”Ÿæˆçš„å·¥ä½œå‡½æ•°ï¼ˆçº¿ç¨‹å·¥ä½œå•å…ƒï¼‰
        
        Args:
            args: (analysis_result, trade_date, date_dir, post_date_dir, skip_existing)
            
        Returns:
            å¸–å­ç”Ÿæˆç»“æœæˆ–None
        """
        analysis_result, trade_date, date_dir, post_date_dir, skip_existing = args
        
        stock_info = analysis_result.get('stock_info', {})
        stock_name = stock_info.get('name', 'Unknown')
        ts_code = stock_info.get('ts_code', 'Unknown')
        thread_id = threading.current_thread().ident
        
        try:
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰å¸–å­
            if skip_existing:
                # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç±»ä¼¼åç§°çš„å¸–å­æ–‡ä»¶
                if os.path.exists(post_date_dir):
                    existing_files = [f for f in os.listdir(post_date_dir) 
                                    if f.startswith(f"{trade_date}_{stock_name}_gushen_post")]
                    if existing_files:
                        logger.debug(f"[çº¿ç¨‹{thread_id}] è·³è¿‡å·²ç”Ÿæˆçš„å¸–å­: {stock_name}")
                        return {
                            "stock_name": stock_name,
                            "ts_code": ts_code,
                            "post_file": os.path.join(post_date_dir, existing_files[0]),
                            "status": "skipped",
                            "generated_at": None
                        }
            
            # ä¿å­˜ä¸´æ—¶åˆ†æç»“æœæ–‡ä»¶ï¼ˆç”¨äºå¸–å­ç”Ÿæˆï¼‰
            temp_analysis_file = os.path.join(date_dir, f"{stock_name}_{ts_code.replace('.', '_')}_analysis.json")
            
            if not os.path.exists(temp_analysis_file):
                logger.warning(f"[çº¿ç¨‹{thread_id}] åˆ†æç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {temp_analysis_file}")
                return None
            
            # è½»å¾®çš„APIè°ƒç”¨å»¶è¿Ÿï¼ˆé˜²æ­¢è¿‡åº¦å¹¶å‘ï¼‰
            if self.api_delay > 0:
                time.sleep(self.api_delay)
            
            # åˆ›å»ºçº¿ç¨‹ä¸“ç”¨çš„å¸–å­ç”Ÿæˆå™¨å®ä¾‹
            post_generator = PostGeneratorV2()
            
            # ç”Ÿæˆå¸–å­å†…å®¹
            try:
                # 1. åŠ è½½æ•°æ®
                analysis_data = post_generator.load_analysis_data(temp_analysis_file)
                
                # 2. ç”Ÿæˆé˜¶æ®µä¸€å†…å®¹
                stage1_content, thinking1 = post_generator.generate_stage1_content(analysis_data)
                
                # 3. ç”Ÿæˆé˜¶æ®µäºŒå†…å®¹
                stage2_content, thinking2 = post_generator.generate_stage2_content(analysis_data, stage1_content)
                
                # 4. åˆå¹¶å†…å®¹
                final_content = post_generator.combine_content(stage1_content, stage2_content)
                
                # 5. ä¿å­˜å®Œæ•´å¸–å­åˆ°æŒ‡å®šçš„æ—¥æœŸç›®å½•
                post_filepath = post_generator.save_post(
                    final_content, 
                    analysis_data,
                    stage1_thinking=thinking1,
                    stage2_json_data=thinking2,
                    output_dir=post_date_dir  # ä½¿ç”¨æ—¥æœŸç›®å½•
                )
                
                post_result = {
                    "success": True,
                    "post_filepath": post_filepath,
                    "final_content": final_content
                }
                
            except Exception as e:
                logger.error(f"[çº¿ç¨‹{thread_id}] å¸–å­ç”Ÿæˆè¿‡ç¨‹å¤±è´¥: {e}")
                post_result = {
                    "success": False,
                    "error": str(e)
                }
            
            if post_result and post_result["success"]:
                result_info = {
                    "stock_name": stock_name,
                    "ts_code": ts_code,
                    "post_file": post_result["post_filepath"],
                    "status": "success",
                    "generated_at": datetime.now().isoformat()
                }
                logger.debug(f"[çº¿ç¨‹{thread_id}] å®Œæˆ{stock_name}å¸–å­ç”Ÿæˆ")
                return result_info
            else:
                error_msg = post_result.get("error", "æœªçŸ¥é”™è¯¯") if post_result else "å¸–å­ç”Ÿæˆå™¨è¿”å›None"
                logger.warning(f"[çº¿ç¨‹{thread_id}] ç”Ÿæˆ{stock_name}å¸–å­å¤±è´¥: {error_msg}")
                return None
                
        except Exception as e:
            logger.error(f"[çº¿ç¨‹{thread_id}] å¤„ç†{stock_name}å¸–å­æ—¶å‡ºé”™: {e}")
            return None

    def _extract_single_stock(self, processed_data: Dict[str, Any], stock_data: Dict[str, Any], trade_date: str) -> Optional[str]:
        """
        æå–å•åªè‚¡ç¥¨çš„æ•°æ®
        """
        try:
            # åˆ›å»ºä¸´æ—¶çš„å®Œæ•´æ•°æ®ç»“æ„
            temp_data = {
                "meta": processed_data["meta"],
                "stocks": [stock_data]
            }
            
            # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
            temp_file = f"data/extracted/temp_{trade_date}_{stock_data['name']}_{stock_data['ts_code'].replace('.', '_')}.json"
            with open(temp_file, 'w', encoding='utf-8') as f:
                json.dump(temp_data, f, ensure_ascii=False, indent=2)
            
            return temp_file
            
        except Exception as e:
            logger.error(f"æå–å•è‚¡æ•°æ®å¤±è´¥: {e}")
            return None
    
    def _analyze_single_stock(self, extracted_file: str, stock_name: str, ts_code: str) -> Optional[Dict[str, Any]]:
        """
        ä½¿ç”¨LLMåˆ†æå•åªè‚¡ç¥¨ï¼ˆå·²å¼ƒç”¨ï¼Œä¿ç•™å…¼å®¹æ€§ï¼‰
        """
        try:
            # åˆ›å»ºåˆ†æå™¨å®ä¾‹
            funding_analyzer = FundingBattleAnalyzer()
            
            # æ‰§è¡Œèµ„é‡‘åšå¼ˆåˆ†æ
            analysis_result = funding_analyzer.analyze_complete_report(
                data_file_path=extracted_file,
                output_path=None  # ä¸é¢å¤–ä¿å­˜ï¼Œç”±ä¸Šå±‚ç»Ÿä¸€ç®¡ç†
            )
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(extracted_file):
                os.remove(extracted_file)
            
            if analysis_result:
                # æ·»åŠ å¤„ç†æ—¶é—´æˆ³
                analysis_result['processed_at'] = datetime.now().isoformat()
                return analysis_result
            
            return None
            
        except Exception as e:
            logger.error(f"LLMåˆ†æ{stock_name}å¤±è´¥: {e}")
            return None
    
    def _step4_generate_summary(self, analysis_results: List[Dict[str, Any]], post_results: List[Dict[str, Any]], trade_date: str) -> Dict[str, Any]:
        """
        æ­¥éª¤4ï¼šç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
        """
        logger.info("æ­¥éª¤4: ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š")
        
        # ç¡®ä¿æ—¥æœŸç›®å½•å­˜åœ¨
        date_dir = self._ensure_date_directory(trade_date)
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_stocks = len(analysis_results)
        successful_analysis = len([r for r in analysis_results if r and 'analysis_report' in r])
        
        # å¸–å­ç”Ÿæˆç»Ÿè®¡
        total_posts = len(post_results)
        successful_posts = len([r for r in post_results if r and r.get('status') == 'success'])
        skipped_posts = len([r for r in post_results if r and r.get('status') == 'skipped'])
        
        # æ„å»ºè½»é‡çº§æ±‡æ€»ç»“æœï¼ˆä¸åŒ…å«å®Œæ•´çš„è‚¡ç¥¨åˆ†ææ•°æ®ï¼ŒåªåŒ…å«ç»Ÿè®¡ä¿¡æ¯ï¼‰
        summary = {
            "meta": {
                "trade_date": trade_date,
                "trade_date_display": self._format_date_display(trade_date),
                "total_stocks": total_stocks,
                "successful_analysis": successful_analysis,
                "analysis_success_rate": f"{(successful_analysis/total_stocks*100):.1f}%" if total_stocks > 0 else "0%",
                "post_generation_enabled": self.enable_post_generation,
                "total_posts": total_posts,
                "successful_posts": successful_posts,
                "skipped_posts": skipped_posts,
                "post_success_rate": f"{(successful_posts/total_posts*100):.1f}%" if total_posts > 0 else "0%",
                "generated_at": datetime.now().isoformat(),
                "processing_mode": "high_concurrency_parallel" if not hasattr(self, '_force_serial') or not self._force_serial else "serial",
                "max_workers": self.max_workers,
                "api_delay": self.api_delay
            },
            "stock_list": [
                {
                    "name": result.get('stock_info', {}).get('name', 'Unknown'),
                    "ts_code": result.get('stock_info', {}).get('ts_code', 'Unknown'),
                    "analysis_status": "success" if result and 'analysis_report' in result else "failed",
                    "post_status": self._get_post_status(result.get('stock_info', {}).get('name', 'Unknown'), post_results),
                    "post_file": self._get_post_file(result.get('stock_info', {}).get('name', 'Unknown'), post_results)
                }
                for result in analysis_results
            ],
            "summary_stats": self._calculate_summary_stats(analysis_results)
        }
        
        # ä¿å­˜æ±‡æ€»æŠ¥å‘Šåˆ°æ—¥æœŸç›®å½•ä¸‹
        summary_file = os.path.join(date_dir, "summary.json")
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        logger.info(f"æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜: {summary_file}")
        logger.info(f"æˆåŠŸåˆ†æ {successful_analysis}/{total_stocks} åªè‚¡ç¥¨")
        if self.enable_post_generation:
            logger.info(f"æˆåŠŸç”Ÿæˆ {successful_posts}/{total_posts} ç¯‡å¸–å­")
            if skipped_posts > 0:
                logger.info(f"è·³è¿‡å·²å­˜åœ¨ {skipped_posts} ç¯‡å¸–å­")
        logger.info(f"æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°ç›®å½•: {date_dir}")
        
        return summary
    
    def _get_post_status(self, stock_name: str, post_results: List[Dict[str, Any]]) -> str:
        """è·å–æŒ‡å®šè‚¡ç¥¨çš„å¸–å­çŠ¶æ€"""
        if not self.enable_post_generation:
            return "disabled"
        
        for post_result in post_results:
            if post_result.get('stock_name') == stock_name:
                return post_result.get('status', 'unknown')
        
        return "not_generated"
    
    def _get_post_file(self, stock_name: str, post_results: List[Dict[str, Any]]) -> Optional[str]:
        """è·å–æŒ‡å®šè‚¡ç¥¨çš„å¸–å­æ–‡ä»¶è·¯å¾„"""
        if not self.enable_post_generation:
            return None
        
        for post_result in post_results:
            if post_result.get('stock_name') == stock_name:
                return post_result.get('post_file')
        
        return None
    
    def _format_date_display(self, date_str: str) -> str:
        """æ ¼å¼åŒ–æ—¥æœŸæ˜¾ç¤º"""
        try:
            if len(date_str) == 8:
                return f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:8]}"
            return date_str
        except:
            return date_str
    
    def _calculate_summary_stats(self, analysis_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è®¡ç®—æ±‡æ€»ç»Ÿè®¡ä¿¡æ¯"""
        if not analysis_results:
            return {}
        
        try:
            # æå–æœ‰æ•ˆçš„åˆ†æç»“æœ
            valid_results = [r for r in analysis_results if r and 'analysis_report' in r]
            
            if not valid_results:
                return {"note": "æ²¡æœ‰æœ‰æ•ˆçš„åˆ†æç»“æœ"}
            
            # ç»Ÿè®¡æˆ˜å±€æ€»è§ˆä¿¡æ¯
            verdicts = []
            confidence_scores = []
            
            for result in valid_results:
                overall_assessment = result.get('analysis_report', {}).get('overall_assessment', {})
                if 'verdict' in overall_assessment:
                    verdicts.append(overall_assessment['verdict'])
                if 'confidence_score' in overall_assessment:
                    try:
                        confidence_scores.append(float(overall_assessment['confidence_score']))
                    except (ValueError, TypeError):
                        pass
            
            stats = {
                "verdict_distribution": self._count_items(verdicts),
                "average_confidence": round(sum(confidence_scores) / len(confidence_scores), 2) if confidence_scores else 0
            }
            
            return stats
            
        except Exception as e:
            logger.error(f"è®¡ç®—æ±‡æ€»ç»Ÿè®¡å¤±è´¥: {e}")
            return {"error": "ç»Ÿè®¡è®¡ç®—å¤±è´¥"}
    
    def _count_items(self, items: List[str]) -> Dict[str, int]:
        """ç»Ÿè®¡é¡¹ç›®å‡ºç°æ¬¡æ•°"""
        count_dict = {}
        for item in items:
            count_dict[item] = count_dict.get(item, 0) + 1
        return count_dict
    
    def batch_process_dates(self, date_list: List[str], skip_existing: bool = True) -> Dict[str, Any]:
        """
        æ‰¹é‡å¤„ç†å¤šä¸ªæ—¥æœŸ
        
        Args:
            date_list: æ—¥æœŸåˆ—è¡¨
            skip_existing: æ˜¯å¦è·³è¿‡å·²å­˜åœ¨çš„ç»“æœ
            
        Returns:
            æ‰¹é‡å¤„ç†ç»“æœæ±‡æ€»
        """
        logger.info(f"å¼€å§‹æ‰¹é‡å¤„ç†{len(date_list)}ä¸ªæ—¥æœŸ")
        
        batch_results = {}
        
        for trade_date in date_list:
            logger.info(f"å¤„ç†æ—¥æœŸ: {trade_date}")
            try:
                result = self.process_date(trade_date, skip_existing)
                batch_results[trade_date] = result
            except Exception as e:
                logger.error(f"å¤„ç†æ—¥æœŸ{trade_date}å¤±è´¥: {e}")
                batch_results[trade_date] = {"error": str(e)}
        
        # ä¿å­˜æ‰¹é‡ç»“æœ
        batch_summary_file = f"data/analyzed/batch_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(batch_summary_file, 'w', encoding='utf-8') as f:
            json.dump(batch_results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"æ‰¹é‡å¤„ç†å®Œæˆï¼Œç»“æœå·²ä¿å­˜: {batch_summary_file}")
        return batch_results


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='é¾™è™æ¦œæ•°æ®å¤„ç†å®Œæ•´æµæ°´çº¿ï¼ˆé«˜å¹¶å‘ç‰ˆæœ¬ï¼‰')
    parser.add_argument('trade_date', help='äº¤æ˜“æ—¥æœŸ (YYYYMMDDæ ¼å¼)')
    parser.add_argument('--token', help='Tushare API token (å¯é€‰)')
    parser.add_argument('--skip-existing', action='store_true', default=True, 
                       help='è·³è¿‡å·²å­˜åœ¨çš„åˆ†æç»“æœ (é»˜è®¤: True)')
    parser.add_argument('--verbose', action='store_true', help='æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—')
    parser.add_argument('--batch', help='æ‰¹é‡å¤„ç†ï¼Œä¼ å…¥åŒ…å«æ—¥æœŸåˆ—è¡¨çš„æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--workers', type=int, default=16, help='æœ€å¤§å¹¶è¡Œçº¿ç¨‹æ•° (é»˜è®¤: 16ï¼Œé€‚åˆDeepSeekæ— é™åˆ¶)')
    parser.add_argument('--delay', type=float, default=0.1, help='APIè°ƒç”¨é—´éš”ç§’æ•° (é»˜è®¤: 0.1ï¼Œå¯ä»¥å¾ˆå°)')
    parser.add_argument('--serial', action='store_true', help='ä½¿ç”¨ä¸²è¡Œæ¨¡å¼ï¼ˆè°ƒè¯•ç”¨ï¼‰')
    parser.add_argument('--enable-posts', action='store_true', help='å¯ç”¨å¸–å­ç”ŸæˆåŠŸèƒ½')
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # åˆå§‹åŒ–æµæ°´çº¿
    pipeline = DragonTigerPipeline(
        tushare_token=args.token,
        max_workers=args.workers,
        api_delay=args.delay,
        enable_post_generation=args.enable_posts
    )
    
    # è®¾ç½®ä¸²è¡Œæ¨¡å¼ï¼ˆå¦‚æœæŒ‡å®šï¼‰
    if args.serial:
        pipeline._force_serial = True
        logger.info("å¼ºåˆ¶ä½¿ç”¨ä¸²è¡Œæ¨¡å¼")
    
    try:
        if args.batch:
            # æ‰¹é‡å¤„ç†æ¨¡å¼
            with open(args.batch, 'r', encoding='utf-8') as f:
                date_list = [line.strip() for line in f if line.strip()]
            
            result = pipeline.batch_process_dates(date_list, args.skip_existing)
            print("æ‰¹é‡å¤„ç†å®Œæˆ")
        else:
            # å•æ—¥å¤„ç†æ¨¡å¼
            result = pipeline.process_date(args.trade_date, args.skip_existing)
            
            if 'error' in result:
                print(f"å¤„ç†å¤±è´¥: {result['error']}")
            else:
                print("="*60)
                print(f"ğŸš€ é¾™è™æ¦œæ•°æ®å¤„ç†å®Œæˆ - {args.trade_date}")
                print("="*60)
                print(f"ğŸ“Š æ€»è‚¡ç¥¨æ•°: {result['meta']['total_stocks']}")
                print(f"âœ… æˆåŠŸåˆ†æ: {result['meta']['successful_analysis']}")
                print(f"ğŸ“ˆ åˆ†ææˆåŠŸç‡: {result['meta']['analysis_success_rate']}")
                
                # å¸–å­ç”Ÿæˆç»Ÿè®¡
                if result['meta']['post_generation_enabled']:
                    print(f"ğŸ“ å¸–å­ç”Ÿæˆ: å¯ç”¨")
                    print(f"ğŸ“„ æˆåŠŸç”Ÿæˆ: {result['meta']['successful_posts']}")
                    print(f"â­ï¸  è·³è¿‡å·²å­˜åœ¨: {result['meta']['skipped_posts']}")
                    print(f"ğŸ“Š å¸–å­æˆåŠŸç‡: {result['meta']['post_success_rate']}")
                else:
                    print(f"ğŸ“ å¸–å­ç”Ÿæˆ: æœªå¯ç”¨")
                
                print(f"âš¡ å¤„ç†æ¨¡å¼: {result['meta']['processing_mode']}")
                print(f"ğŸ”¥ å¹¶è¡Œçº¿ç¨‹: {result['meta']['max_workers']}")
                print(f"â±ï¸  APIå»¶è¿Ÿ: {result['meta']['api_delay']}ç§’")
                print(f"ğŸ“ ç»“æœç›®å½•: data/analyzed/{args.trade_date}/")
                
                print("ğŸ“‚ æ–‡ä»¶ç»“æ„:")
                print(f"  â”œâ”€â”€ summary.json  (æ±‡æ€»ç»Ÿè®¡)")
                print(f"  â”œâ”€â”€ è‚¡ç¥¨åç§°1_ä»£ç _analysis.json")
                print(f"  â”œâ”€â”€ è‚¡ç¥¨åç§°2_ä»£ç _analysis.json")
                print(f"  â””â”€â”€ ...")
                
                if result['meta']['post_generation_enabled']:
                    print(f"ğŸ“„ å¸–å­æ–‡ä»¶: data/output/posts/{args.trade_date}/")
                    print(f"  â”œâ”€â”€ {args.trade_date}_è‚¡ç¥¨åç§°1_gushen_post_v2.1_HHMMSS.md")
                    print(f"  â”œâ”€â”€ {args.trade_date}_è‚¡ç¥¨åç§°2_gushen_post_v2.1_HHMMSS.md")
                    print(f"  â””â”€â”€ ...")
                
                print(f"\nğŸ¯ ä½¿ç”¨é«˜å¹¶å‘å¤„ç†ï¼Œå¤§å¹…æå‡æ•ˆç‡ï¼")
                if result['meta']['post_generation_enabled']:
                    print(f"ğŸ’¡ ç°åœ¨æ”¯æŒä¸€é”®ç”ŸæˆæŠ•èµ„å¸–å­ï¼")
    
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­å¤„ç†")
        print("\nå¤„ç†å·²ä¸­æ–­")
    except Exception as e:
        logger.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print(f"é”™è¯¯: {e}")


if __name__ == "__main__":
    main() 